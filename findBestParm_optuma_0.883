{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled74.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOOBscQAAXlG7ANqrofl6iY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mingun0112/GestureClassification/blob/main/findBestParm_optuma_0.883\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0NwtJaGcOnC",
        "outputId": "f72bca44-2ab1-404e-b912-49fc847c0d5c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna) (3.10.1)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.6)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.32)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.63.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (3.13)\n",
            "Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from optuna) (0.8.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna) (6.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.5)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.7)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.11.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (1.2.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.4.0)\n",
            "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.5.0)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.2.0)\n",
            "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (2.4.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (5.8.1)\n",
            "Requirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (0.5.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (3.10.0.2)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7T4ALz7uar8E"
      },
      "outputs": [],
      "source": [
        "# Basic Library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# HP Tuning\n",
        "import optuna\n",
        "from optuna import Trial\n",
        "from optuna.samplers import TPESampler\n",
        "from optuna.visualization import plot_contour, plot_optimization_history\n",
        "from optuna.visualization import plot_parallel_coordinate, plot_slice, plot_param_importances\n",
        "\n",
        "# Modeling\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Dropout, Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpYNdWCeaud9",
        "outputId": "bd867395-5708-48c8-a244-9b103f266cf0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/hand_gesture_data.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-6wMLUFbqHQ",
        "outputId": "5d0a45ee-51f3-4f32-ecd8-f2b9c7e8b8f9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/hand_gesture_data.zip\n",
            "replace sample_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train=pd.read_csv(\"/content/train.csv\")\n",
        "test=pd.read_csv(\"/content/test.csv\")\n",
        "submission=pd.read_csv(\"/content/sample_submission.csv\")"
      ],
      "metadata": {
        "id": "YNCKmQFqbVj7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train.iloc[:, 1:-1]\n",
        "test_x = test.iloc[:, 1:]\n",
        "\n",
        "train_x = np.array(train_x).reshape(-1, 8, 4, 1)\n",
        "test_x = np.array(test_x).reshape(-1, 8, 4, 1)\n",
        "\n",
        "ohe = OneHotEncoder(sparse = False)\n",
        "train_y = ohe.fit_transform(train[['target']])"
      ],
      "metadata": {
        "id": "PJLvfcZncqwy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(num_layer, mid_units, num_filters):\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters=num_filters[0], kernel_size=(2, 2),\n",
        "                 activation=\"relu\",\n",
        "                 input_shape=(8, 4, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    #model.add(Dropout(dropout_rate[0]))\n",
        "    for i in range(1,num_layer):\n",
        "        model.add(Conv2D(filters=num_filters[i], kernel_size=(2, 2), padding=\"same\", activation=\"relu\"))\n",
        "        model.add(BatchNormalization())\n",
        "        #model.add(Dropout(dropout_rate[i+1]))\n",
        "            \n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dense(mid_units, activation='relu'))\n",
        "    #model.add(Dropout(dropout_rate[-1]))\n",
        "    model.add(Dense(4, activation='softmax'))\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "Ciyl2LUWcxDG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cnn_objective(trial: Trial) -> float:\n",
        "    \n",
        "    #clear_session\n",
        "    keras.backend.clear_session()\n",
        "    \n",
        "    #number of the convolution layer\n",
        "    num_layer = trial.suggest_int(\"num_layer\", 2, 3)\n",
        "    \n",
        "    #number of the unit\n",
        "    mid_units = int(trial.suggest_discrete_uniform(\"mid_units\", 30, 150, 10))\n",
        "    \n",
        "    #number of the each convolution layer filter\n",
        "    num_filters = [int(trial.suggest_discrete_uniform(\"num_filter_\"+str(i), 16, 256, 16)) for i in range(num_layer)]\n",
        "\n",
        "    #Dropout\n",
        "    #dropout_rate = trial.suggest_uniform('dropout_rate', 0.0, 0.5)\n",
        "    #dropout_rate = [int(trial.suggest_uniform(\"dropout_rate\"+str(ii), 0.0, 0.5)) for ii in range(num_layer+1)]\n",
        "    \n",
        "        \n",
        "    seed = 42\n",
        "    kfold = StratifiedKFold(n_splits=10, random_state = seed, shuffle = True) # Cross-validation cv=5\n",
        "    es = EarlyStopping(monitor=\"val_acc\", patience=5, mode=\"max\", verbose=0)\n",
        "    cv = np.zeros((train_x.shape[0], 4))\n",
        "\n",
        "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train.target)):\n",
        "\n",
        "        x_train, y_train = train_x[train_idx], train_y[train_idx]\n",
        "        x_val, y_val = train_x[val_idx], train_y[val_idx]\n",
        "        \n",
        "        mc = ModelCheckpoint(f\"model_{n+1}.h5\", save_best_only=True, monitor=\"val_acc\", mode=\"max\", verbose=0)\n",
        "        \n",
        "        model = create_model(num_layer, mid_units, num_filters)\n",
        "        \n",
        "        # Adam optimizer learning rate\n",
        "        optimizer = Adam(learning_rate=trial.suggest_uniform(\"learning_rate\", 0.0005, 0.005))\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=\"categorical_crossentropy\",\n",
        "                      metrics=[\"acc\"])\n",
        "        \n",
        "        model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=100, batch_size=32, \n",
        "                  callbacks=[es,mc], verbose=0)\n",
        "        \n",
        "        best = load_model(f\"model_{n+1}.h5\")\n",
        "        \n",
        "        cv[val_idx, :] = best.predict(x_val)\n",
        "        \n",
        "    print('multi_logloss:', log_loss(train_y, cv))\n",
        "    print('accuracy_score:', accuracy_score(np.argmax(train_y, axis=1), np.argmax(cv, axis=1)))\n",
        "\n",
        "    \n",
        "    return accuracy_score(np.argmax(train_y, axis=1), np.argmax(cv, axis=1))"
      ],
      "metadata": {
        "id": "CtdDY_sidUtd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = TPESampler(seed=42)\n",
        "cnn_study = optuna.create_study(study_name=\"cnn_parameter_opt\", direction=\"maximize\", sampler=sampler)\n",
        "cnn_study.optimize(cnn_objective, n_trials=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej3nau2sd1qU",
        "outputId": "c90db8ab-09d9-4a77-e1ca-91cf1bb0325d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-03-14 08:15:32,714]\u001b[0m A new study created in memory with name: cnn_parameter_opt\u001b[0m\n",
            "\u001b[32m[I 2022-03-14 08:21:15,055]\u001b[0m Trial 0 finished with value: 0.8796573875802998 and parameters: {'num_layer': 2, 'mid_units': 150.0, 'num_filter_0': 192.0, 'num_filter_1': 160.0, 'learning_rate': 0.0012020838819909643}. Best is trial 0 with value: 0.8796573875802998.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multi_logloss: 0.3651059863068586\n",
            "accuracy_score: 0.8796573875802998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-03-14 08:26:50,212]\u001b[0m Trial 1 finished with value: 0.880085653104925 and parameters: {'num_layer': 2, 'mid_units': 30.0, 'num_filter_0': 224.0, 'num_filter_1': 160.0, 'learning_rate': 0.003686326600082205}. Best is trial 1 with value: 0.880085653104925.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multi_logloss: 0.3654894503719491\n",
            "accuracy_score: 0.880085653104925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-03-14 08:29:35,996]\u001b[0m Trial 2 finished with value: 0.8736616702355461 and parameters: {'num_layer': 2, 'mid_units': 150.0, 'num_filter_0': 224.0, 'num_filter_1': 64.0, 'learning_rate': 0.001318212352431953}. Best is trial 1 with value: 0.880085653104925.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multi_logloss: 0.37054761757989696\n",
            "accuracy_score: 0.8736616702355461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-03-14 08:32:12,132]\u001b[0m Trial 3 finished with value: 0.880085653104925 and parameters: {'num_layer': 2, 'mid_units': 60.0, 'num_filter_0': 144.0, 'num_filter_1': 112.0, 'learning_rate': 0.0018105311308911887}. Best is trial 1 with value: 0.880085653104925.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multi_logloss: 0.36486126093431165\n",
            "accuracy_score: 0.880085653104925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-03-14 08:35:06,514]\u001b[0m Trial 4 finished with value: 0.8706638115631692 and parameters: {'num_layer': 3, 'mid_units': 40.0, 'num_filter_0': 80.0, 'num_filter_1': 96.0, 'num_filter_2': 128.0, 'learning_rate': 0.004033291826268562}. Best is trial 1 with value: 0.880085653104925.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multi_logloss: 0.4130699960207406\n",
            "accuracy_score: 0.8706638115631692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-03-14 08:37:26,225]\u001b[0m Trial 5 finished with value: 0.878372591006424 and parameters: {'num_layer': 2, 'mid_units': 90.0, 'num_filter_0': 160.0, 'num_filter_1': 16.0, 'learning_rate': 0.003233951833556473}. Best is trial 1 with value: 0.880085653104925.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multi_logloss: 0.3709303872011324\n",
            "accuracy_score: 0.878372591006424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-03-14 08:44:37,828]\u001b[0m Trial 6 finished with value: 0.871948608137045 and parameters: {'num_layer': 2, 'mid_units': 30.0, 'num_filter_0': 256.0, 'num_filter_1': 256.0, 'learning_rate': 0.004137788066524076}. Best is trial 1 with value: 0.880085653104925.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multi_logloss: 0.404989634729532\n",
            "accuracy_score: 0.871948608137045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-03-14 08:47:32,346]\u001b[0m Trial 7 finished with value: 0.8805139186295503 and parameters: {'num_layer': 2, 'mid_units': 40.0, 'num_filter_0': 176.0, 'num_filter_1': 128.0, 'learning_rate': 0.0010491720568015048}. Best is trial 7 with value: 0.8805139186295503.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multi_logloss: 0.3685863611016149\n",
            "accuracy_score: 0.8805139186295503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-03-14 08:50:28,839]\u001b[0m Trial 8 finished with value: 0.8779443254817987 and parameters: {'num_layer': 2, 'mid_units': 30.0, 'num_filter_0': 240.0, 'num_filter_1': 80.0, 'learning_rate': 0.0034813502795929194}. Best is trial 7 with value: 0.8805139186295503.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multi_logloss: 0.3719153898450994\n",
            "accuracy_score: 0.8779443254817987\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-03-14 08:52:26,824]\u001b[0m Trial 9 finished with value: 0.8792291220556745 and parameters: {'num_layer': 2, 'mid_units': 90.0, 'num_filter_0': 144.0, 'num_filter_1': 48.0, 'learning_rate': 0.004863130824940514}. Best is trial 7 with value: 0.8805139186295503.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multi_logloss: 0.37664379396183006\n",
            "accuracy_score: 0.8792291220556745\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-03-14 08:58:57,508]\u001b[0m Trial 10 finished with value: 0.8663811563169165 and parameters: {'num_layer': 3, 'mid_units': 120.0, 'num_filter_0': 16.0, 'num_filter_1': 224.0, 'num_filter_2': 256.0, 'learning_rate': 0.00232892986962319}. Best is trial 7 with value: 0.8805139186295503.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multi_logloss: 0.4282513753686166\n",
            "accuracy_score: 0.8663811563169165\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-03-14 09:04:30,075]\u001b[0m Trial 11 finished with value: 0.8728051391862955 and parameters: {'num_layer': 3, 'mid_units': 60.0, 'num_filter_0': 192.0, 'num_filter_1': 176.0, 'num_filter_2': 16.0, 'learning_rate': 0.000579132649020539}. Best is trial 7 with value: 0.8805139186295503.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multi_logloss: 0.39791661988606764\n",
            "accuracy_score: 0.8728051391862955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-03-14 09:06:44,332]\u001b[0m Trial 12 finished with value: 0.8775160599571734 and parameters: {'num_layer': 2, 'mid_units': 60.0, 'num_filter_0': 96.0, 'num_filter_1': 144.0, 'learning_rate': 0.0026547919681696457}. Best is trial 7 with value: 0.8805139186295503.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multi_logloss: 0.38012850742670096\n",
            "accuracy_score: 0.8775160599571734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-03-14 09:12:48,692]\u001b[0m Trial 13 finished with value: 0.8770877944325481 and parameters: {'num_layer': 2, 'mid_units': 50.0, 'num_filter_0': 192.0, 'num_filter_1': 192.0, 'learning_rate': 0.0005585811489823656}. Best is trial 7 with value: 0.8805139186295503.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multi_logloss: 0.3721720593842879\n",
            "accuracy_score: 0.8770877944325481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-03-14 09:19:21,900]\u001b[0m Trial 14 finished with value: 0.8668094218415417 and parameters: {'num_layer': 3, 'mid_units': 70.0, 'num_filter_0': 96.0, 'num_filter_1': 128.0, 'num_filter_2': 256.0, 'learning_rate': 0.003338603399853998}. Best is trial 7 with value: 0.8805139186295503.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multi_logloss: 0.4354043542191099\n",
            "accuracy_score: 0.8668094218415417\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-03-14 09:25:25,189]\u001b[0m Trial 15 finished with value: 0.8779443254817987 and parameters: {'num_layer': 2, 'mid_units': 110.0, 'num_filter_0': 208.0, 'num_filter_1': 224.0, 'learning_rate': 0.0021499489796574928}. Best is trial 7 with value: 0.8805139186295503.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multi_logloss: 0.37624358211484765\n",
            "accuracy_score: 0.8779443254817987\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-03-14 09:29:50,569]\u001b[0m Trial 16 finished with value: 0.8650963597430407 and parameters: {'num_layer': 2, 'mid_units': 30.0, 'num_filter_0': 176.0, 'num_filter_1': 192.0, 'learning_rate': 0.004844020757294549}. Best is trial 7 with value: 0.8805139186295503.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multi_logloss: 0.39087476099910334\n",
            "accuracy_score: 0.8650963597430407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-03-14 09:35:46,730]\u001b[0m Trial 17 finished with value: 0.8736616702355461 and parameters: {'num_layer': 3, 'mid_units': 80.0, 'num_filter_0': 256.0, 'num_filter_1': 128.0, 'num_filter_2': 32.0, 'learning_rate': 0.004054845598390999}. Best is trial 7 with value: 0.8805139186295503.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multi_logloss: 0.3886386371921793\n",
            "accuracy_score: 0.8736616702355461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-03-14 09:38:31,950]\u001b[0m Trial 18 finished with value: 0.8817987152034261 and parameters: {'num_layer': 2, 'mid_units': 50.0, 'num_filter_0': 112.0, 'num_filter_1': 160.0, 'learning_rate': 0.002847819696189126}. Best is trial 18 with value: 0.8817987152034261.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multi_logloss: 0.3668508801768939\n",
            "accuracy_score: 0.8817987152034261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-03-14 09:40:00,963]\u001b[0m Trial 19 finished with value: 0.8736616702355461 and parameters: {'num_layer': 2, 'mid_units': 50.0, 'num_filter_0': 48.0, 'num_filter_1': 96.0, 'learning_rate': 0.0029549628500393703}. Best is trial 18 with value: 0.8817987152034261.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multi_logloss: 0.3735963947053439\n",
            "accuracy_score: 0.8736616702355461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_acc = []\n",
        "cnn_pred = np.zeros((test_x.shape[0], 4))\n",
        "seed = 42\n",
        "skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
        "es = EarlyStopping(monitor=\"val_acc\", patience=30, mode=\"max\", verbose=0)\n",
        "\n",
        "for i, (train_idx, val_idx) in enumerate(skf.split(train_x, train.target)):\n",
        "    print(f\"{i+1} Fold Training.....\")\n",
        "    x_train, y_train = train_x[train_idx], train_y[train_idx]\n",
        "    x_val, y_val = train_x[val_idx], train_y[val_idx]\n",
        "    \n",
        "    cnn = create_model(cnn_study.best_params['num_layer'], cnn_study.best_params['mid_units'], \n",
        "                      [cnn_study.best_params[f'num_filter_{i}'] for i in range(cnn_study.best_params['num_layer'])])\n",
        "    \n",
        "    # ModelCheckpoint Fold마다 갱신\n",
        "    mc = ModelCheckpoint(f\"model_{i+1}.h5\", save_best_only=True, monitor=\"val_acc\", mode=\"max\", verbose=0)\n",
        "    \n",
        "    # 모델 Complie\n",
        "    optimizer = Adam(learning_rate=cnn_study.best_params['learning_rate'])\n",
        "    cnn.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
        "    cnn.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=100, batch_size=32, callbacks=[es,mc], verbose=1)\n",
        "    \n",
        "    # 최고 성능 기록 모델\n",
        "    best = load_model(f\"model_{i+1}.h5\")\n",
        "    \n",
        "    val_pred = best.predict(x_val)\n",
        "    \n",
        "    val_cls = np.argmax(val_pred, axis=1)\n",
        "    \n",
        "    fold_cnn_acc = accuracy_score(np.argmax(y_val, axis=1), val_cls)\n",
        "    cnn_acc.append(fold_cnn_acc)\n",
        "    print(f\"{i+1} Fold ACC of CNN = {fold_cnn_acc}\\n\")\n",
        "    \n",
        "    fold_pred = best.predict(test_x) / skf.n_splits\n",
        "    cnn_pred += fold_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkP33_sVfA-Q",
        "outputId": "c44f0ee3-1247-434f-d940-e54f845bb932"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Fold Training.....\n",
            "Epoch 1/100\n",
            "66/66 [==============================] - 2s 19ms/step - loss: 0.7210 - acc: 0.7149 - val_loss: 0.9327 - val_acc: 0.6026\n",
            "Epoch 2/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.4817 - acc: 0.8101 - val_loss: 0.6717 - val_acc: 0.7521\n",
            "Epoch 3/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.4013 - acc: 0.8463 - val_loss: 0.5129 - val_acc: 0.7821\n",
            "Epoch 4/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.3730 - acc: 0.8639 - val_loss: 0.3612 - val_acc: 0.8462\n",
            "Epoch 5/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.3456 - acc: 0.8767 - val_loss: 0.3836 - val_acc: 0.8632\n",
            "Epoch 6/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.3226 - acc: 0.8820 - val_loss: 0.4425 - val_acc: 0.8419\n",
            "Epoch 7/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.3113 - acc: 0.8858 - val_loss: 0.3814 - val_acc: 0.8504\n",
            "Epoch 8/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.3040 - acc: 0.8977 - val_loss: 0.3897 - val_acc: 0.8547\n",
            "Epoch 9/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.3148 - acc: 0.8786 - val_loss: 0.4779 - val_acc: 0.7991\n",
            "Epoch 10/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2776 - acc: 0.8972 - val_loss: 0.3572 - val_acc: 0.8291\n",
            "Epoch 11/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2780 - acc: 0.8953 - val_loss: 0.4170 - val_acc: 0.8419\n",
            "Epoch 12/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2612 - acc: 0.9072 - val_loss: 0.5637 - val_acc: 0.7906\n",
            "Epoch 13/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2711 - acc: 0.8920 - val_loss: 0.3713 - val_acc: 0.8333\n",
            "Epoch 14/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2349 - acc: 0.9129 - val_loss: 0.3928 - val_acc: 0.8462\n",
            "Epoch 15/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2169 - acc: 0.9158 - val_loss: 0.4099 - val_acc: 0.8675\n",
            "Epoch 16/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2208 - acc: 0.9119 - val_loss: 0.4350 - val_acc: 0.8547\n",
            "Epoch 17/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1932 - acc: 0.9315 - val_loss: 0.4320 - val_acc: 0.8504\n",
            "Epoch 18/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2140 - acc: 0.9119 - val_loss: 0.4638 - val_acc: 0.7949\n",
            "Epoch 19/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.1734 - acc: 0.9310 - val_loss: 0.4958 - val_acc: 0.8034\n",
            "Epoch 20/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1826 - acc: 0.9372 - val_loss: 0.4629 - val_acc: 0.8333\n",
            "Epoch 21/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.1784 - acc: 0.9324 - val_loss: 0.4721 - val_acc: 0.8419\n",
            "Epoch 22/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1639 - acc: 0.9300 - val_loss: 0.5520 - val_acc: 0.8162\n",
            "Epoch 23/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1673 - acc: 0.9348 - val_loss: 0.4404 - val_acc: 0.8632\n",
            "Epoch 24/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.1517 - acc: 0.9500 - val_loss: 0.4575 - val_acc: 0.8248\n",
            "Epoch 25/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.1446 - acc: 0.9448 - val_loss: 0.4815 - val_acc: 0.8120\n",
            "Epoch 26/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.1416 - acc: 0.9495 - val_loss: 0.6094 - val_acc: 0.7863\n",
            "Epoch 27/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1478 - acc: 0.9453 - val_loss: 0.5160 - val_acc: 0.8419\n",
            "Epoch 28/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1262 - acc: 0.9567 - val_loss: 0.4643 - val_acc: 0.8547\n",
            "Epoch 29/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1220 - acc: 0.9548 - val_loss: 0.4673 - val_acc: 0.8333\n",
            "Epoch 30/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.1328 - acc: 0.9505 - val_loss: 0.5540 - val_acc: 0.8333\n",
            "Epoch 31/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.1182 - acc: 0.9576 - val_loss: 0.5573 - val_acc: 0.8291\n",
            "Epoch 32/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.1110 - acc: 0.9605 - val_loss: 0.4879 - val_acc: 0.8205\n",
            "Epoch 33/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.1045 - acc: 0.9629 - val_loss: 0.5806 - val_acc: 0.8291\n",
            "Epoch 34/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0993 - acc: 0.9614 - val_loss: 0.4997 - val_acc: 0.8419\n",
            "Epoch 35/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0696 - acc: 0.9752 - val_loss: 0.4934 - val_acc: 0.8376\n",
            "Epoch 36/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.0964 - acc: 0.9648 - val_loss: 0.6207 - val_acc: 0.7991\n",
            "Epoch 37/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1398 - acc: 0.9495 - val_loss: 0.8220 - val_acc: 0.7863\n",
            "Epoch 38/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0875 - acc: 0.9662 - val_loss: 0.6540 - val_acc: 0.8333\n",
            "Epoch 39/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.0898 - acc: 0.9714 - val_loss: 0.6041 - val_acc: 0.8291\n",
            "Epoch 40/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.0744 - acc: 0.9767 - val_loss: 0.6518 - val_acc: 0.8077\n",
            "Epoch 41/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.1142 - acc: 0.9600 - val_loss: 0.7296 - val_acc: 0.7991\n",
            "Epoch 42/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.1289 - acc: 0.9491 - val_loss: 0.7365 - val_acc: 0.8077\n",
            "Epoch 43/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.0977 - acc: 0.9638 - val_loss: 0.5241 - val_acc: 0.8333\n",
            "Epoch 44/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0576 - acc: 0.9805 - val_loss: 0.6005 - val_acc: 0.8462\n",
            "Epoch 45/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.0664 - acc: 0.9786 - val_loss: 0.7592 - val_acc: 0.8120\n",
            "1 Fold ACC of CNN = 0.8675213675213675\n",
            "\n",
            "2 Fold Training.....\n",
            "Epoch 1/100\n",
            "66/66 [==============================] - 2s 18ms/step - loss: 0.7043 - acc: 0.7354 - val_loss: 0.9992 - val_acc: 0.5513\n",
            "Epoch 2/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.4557 - acc: 0.8263 - val_loss: 0.6637 - val_acc: 0.7350\n",
            "Epoch 3/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.4273 - acc: 0.8386 - val_loss: 0.5090 - val_acc: 0.8248\n",
            "Epoch 4/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.3882 - acc: 0.8553 - val_loss: 0.5565 - val_acc: 0.8162\n",
            "Epoch 5/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.3332 - acc: 0.8743 - val_loss: 0.4903 - val_acc: 0.8419\n",
            "Epoch 6/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.3243 - acc: 0.8858 - val_loss: 0.4924 - val_acc: 0.8462\n",
            "Epoch 7/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.3210 - acc: 0.8820 - val_loss: 0.4683 - val_acc: 0.8419\n",
            "Epoch 8/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2986 - acc: 0.8986 - val_loss: 0.5063 - val_acc: 0.8419\n",
            "Epoch 9/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2702 - acc: 0.8972 - val_loss: 0.4964 - val_acc: 0.8291\n",
            "Epoch 10/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2936 - acc: 0.8839 - val_loss: 0.4912 - val_acc: 0.8333\n",
            "Epoch 11/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2737 - acc: 0.8986 - val_loss: 0.5750 - val_acc: 0.8077\n",
            "Epoch 12/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2452 - acc: 0.9081 - val_loss: 0.4400 - val_acc: 0.8504\n",
            "Epoch 13/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2417 - acc: 0.9167 - val_loss: 0.4886 - val_acc: 0.8333\n",
            "Epoch 14/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2264 - acc: 0.9158 - val_loss: 0.5690 - val_acc: 0.8120\n",
            "Epoch 15/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2302 - acc: 0.9081 - val_loss: 0.5191 - val_acc: 0.8291\n",
            "Epoch 16/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2163 - acc: 0.9167 - val_loss: 0.5437 - val_acc: 0.8162\n",
            "Epoch 17/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2146 - acc: 0.9200 - val_loss: 0.5348 - val_acc: 0.8376\n",
            "Epoch 18/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.1945 - acc: 0.9286 - val_loss: 0.5881 - val_acc: 0.8120\n",
            "Epoch 19/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2194 - acc: 0.9105 - val_loss: 0.5752 - val_acc: 0.8419\n",
            "Epoch 20/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1812 - acc: 0.9372 - val_loss: 0.5170 - val_acc: 0.8333\n",
            "Epoch 21/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1743 - acc: 0.9367 - val_loss: 0.6000 - val_acc: 0.8120\n",
            "Epoch 22/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1654 - acc: 0.9343 - val_loss: 0.5434 - val_acc: 0.8376\n",
            "Epoch 23/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1879 - acc: 0.9334 - val_loss: 0.5354 - val_acc: 0.8248\n",
            "Epoch 24/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1636 - acc: 0.9367 - val_loss: 0.5141 - val_acc: 0.8547\n",
            "Epoch 25/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1535 - acc: 0.9448 - val_loss: 0.6231 - val_acc: 0.8162\n",
            "Epoch 26/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1345 - acc: 0.9481 - val_loss: 0.5168 - val_acc: 0.8291\n",
            "Epoch 27/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.1298 - acc: 0.9505 - val_loss: 0.5588 - val_acc: 0.8077\n",
            "Epoch 28/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1117 - acc: 0.9572 - val_loss: 0.7613 - val_acc: 0.7949\n",
            "Epoch 29/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1403 - acc: 0.9467 - val_loss: 0.7051 - val_acc: 0.8291\n",
            "Epoch 30/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1401 - acc: 0.9438 - val_loss: 0.6615 - val_acc: 0.8462\n",
            "Epoch 31/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1294 - acc: 0.9519 - val_loss: 0.7576 - val_acc: 0.7991\n",
            "Epoch 32/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0987 - acc: 0.9624 - val_loss: 0.6118 - val_acc: 0.8462\n",
            "Epoch 33/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1185 - acc: 0.9576 - val_loss: 0.6361 - val_acc: 0.8248\n",
            "Epoch 34/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1001 - acc: 0.9610 - val_loss: 0.7453 - val_acc: 0.7949\n",
            "Epoch 35/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1161 - acc: 0.9581 - val_loss: 0.6699 - val_acc: 0.8291\n",
            "Epoch 36/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1113 - acc: 0.9572 - val_loss: 0.6412 - val_acc: 0.8333\n",
            "Epoch 37/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0969 - acc: 0.9643 - val_loss: 0.5875 - val_acc: 0.8419\n",
            "Epoch 38/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.0692 - acc: 0.9757 - val_loss: 0.6494 - val_acc: 0.8291\n",
            "Epoch 39/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0651 - acc: 0.9805 - val_loss: 0.7348 - val_acc: 0.8291\n",
            "Epoch 40/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0788 - acc: 0.9752 - val_loss: 0.7458 - val_acc: 0.8291\n",
            "Epoch 41/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0796 - acc: 0.9700 - val_loss: 0.6870 - val_acc: 0.8291\n",
            "Epoch 42/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0860 - acc: 0.9638 - val_loss: 0.6575 - val_acc: 0.8419\n",
            "Epoch 43/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0898 - acc: 0.9686 - val_loss: 0.8888 - val_acc: 0.8120\n",
            "Epoch 44/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1175 - acc: 0.9534 - val_loss: 0.8785 - val_acc: 0.7949\n",
            "Epoch 45/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1095 - acc: 0.9614 - val_loss: 0.7404 - val_acc: 0.8291\n",
            "Epoch 46/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0939 - acc: 0.9662 - val_loss: 0.7841 - val_acc: 0.8205\n",
            "Epoch 47/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0638 - acc: 0.9781 - val_loss: 0.8250 - val_acc: 0.8120\n",
            "Epoch 48/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0615 - acc: 0.9772 - val_loss: 0.7664 - val_acc: 0.8120\n",
            "Epoch 49/100\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.0617 - acc: 0.9791 - val_loss: 0.8704 - val_acc: 0.8034\n",
            "Epoch 50/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0600 - acc: 0.9762 - val_loss: 0.8319 - val_acc: 0.8205\n",
            "Epoch 51/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0476 - acc: 0.9824 - val_loss: 0.9548 - val_acc: 0.8034\n",
            "Epoch 52/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0359 - acc: 0.9886 - val_loss: 0.8909 - val_acc: 0.7991\n",
            "Epoch 53/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0355 - acc: 0.9905 - val_loss: 0.8194 - val_acc: 0.8205\n",
            "Epoch 54/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0429 - acc: 0.9881 - val_loss: 0.8178 - val_acc: 0.7906\n",
            "2 Fold ACC of CNN = 0.8547008547008547\n",
            "\n",
            "3 Fold Training.....\n",
            "Epoch 1/100\n",
            "66/66 [==============================] - 2s 18ms/step - loss: 0.8007 - acc: 0.6859 - val_loss: 1.7579 - val_acc: 0.3974\n",
            "Epoch 2/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.4846 - acc: 0.8196 - val_loss: 0.5533 - val_acc: 0.7607\n",
            "Epoch 3/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.4122 - acc: 0.8439 - val_loss: 0.4688 - val_acc: 0.8162\n",
            "Epoch 4/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.4041 - acc: 0.8377 - val_loss: 0.2847 - val_acc: 0.8974\n",
            "Epoch 5/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.3862 - acc: 0.8567 - val_loss: 0.3674 - val_acc: 0.8419\n",
            "Epoch 6/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.3427 - acc: 0.8710 - val_loss: 0.3237 - val_acc: 0.8675\n",
            "Epoch 7/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.3402 - acc: 0.8663 - val_loss: 0.3365 - val_acc: 0.8761\n",
            "Epoch 8/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.3062 - acc: 0.8858 - val_loss: 0.3206 - val_acc: 0.8803\n",
            "Epoch 9/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.3042 - acc: 0.8791 - val_loss: 0.3384 - val_acc: 0.8718\n",
            "Epoch 10/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2841 - acc: 0.8929 - val_loss: 0.3970 - val_acc: 0.8718\n",
            "Epoch 11/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2800 - acc: 0.8939 - val_loss: 0.4163 - val_acc: 0.8547\n",
            "Epoch 12/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.3004 - acc: 0.8858 - val_loss: 0.3506 - val_acc: 0.9017\n",
            "Epoch 13/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2755 - acc: 0.8958 - val_loss: 0.4675 - val_acc: 0.8504\n",
            "Epoch 14/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2434 - acc: 0.9115 - val_loss: 0.3272 - val_acc: 0.8675\n",
            "Epoch 15/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2295 - acc: 0.9115 - val_loss: 0.2735 - val_acc: 0.9103\n",
            "Epoch 16/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2134 - acc: 0.9196 - val_loss: 0.2963 - val_acc: 0.8932\n",
            "Epoch 17/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1953 - acc: 0.9257 - val_loss: 0.4020 - val_acc: 0.8462\n",
            "Epoch 18/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2296 - acc: 0.9158 - val_loss: 0.3711 - val_acc: 0.8846\n",
            "Epoch 19/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2099 - acc: 0.9196 - val_loss: 0.4180 - val_acc: 0.8376\n",
            "Epoch 20/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1883 - acc: 0.9243 - val_loss: 0.3421 - val_acc: 0.8632\n",
            "Epoch 21/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2012 - acc: 0.9243 - val_loss: 0.4004 - val_acc: 0.8419\n",
            "Epoch 22/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1738 - acc: 0.9386 - val_loss: 0.3925 - val_acc: 0.8632\n",
            "Epoch 23/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1799 - acc: 0.9338 - val_loss: 0.3804 - val_acc: 0.8932\n",
            "Epoch 24/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1898 - acc: 0.9262 - val_loss: 0.5537 - val_acc: 0.8462\n",
            "Epoch 25/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2088 - acc: 0.9196 - val_loss: 0.3885 - val_acc: 0.8803\n",
            "Epoch 26/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1550 - acc: 0.9424 - val_loss: 0.2789 - val_acc: 0.8974\n",
            "Epoch 27/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1607 - acc: 0.9429 - val_loss: 0.3882 - val_acc: 0.8547\n",
            "Epoch 28/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1548 - acc: 0.9372 - val_loss: 0.3018 - val_acc: 0.8889\n",
            "Epoch 29/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1262 - acc: 0.9548 - val_loss: 0.4693 - val_acc: 0.8632\n",
            "Epoch 30/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1293 - acc: 0.9505 - val_loss: 0.3409 - val_acc: 0.8932\n",
            "Epoch 31/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0967 - acc: 0.9705 - val_loss: 0.4164 - val_acc: 0.8632\n",
            "Epoch 32/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1096 - acc: 0.9591 - val_loss: 0.4448 - val_acc: 0.8590\n",
            "Epoch 33/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1263 - acc: 0.9576 - val_loss: 0.4210 - val_acc: 0.8675\n",
            "Epoch 34/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1028 - acc: 0.9657 - val_loss: 0.3863 - val_acc: 0.8761\n",
            "Epoch 35/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1179 - acc: 0.9586 - val_loss: 0.4541 - val_acc: 0.8462\n",
            "Epoch 36/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1003 - acc: 0.9653 - val_loss: 0.3264 - val_acc: 0.8846\n",
            "Epoch 37/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0919 - acc: 0.9672 - val_loss: 0.4524 - val_acc: 0.8504\n",
            "Epoch 38/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1154 - acc: 0.9562 - val_loss: 0.6351 - val_acc: 0.8376\n",
            "Epoch 39/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1038 - acc: 0.9657 - val_loss: 0.4851 - val_acc: 0.8462\n",
            "Epoch 40/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0748 - acc: 0.9733 - val_loss: 0.4708 - val_acc: 0.8718\n",
            "Epoch 41/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0980 - acc: 0.9634 - val_loss: 0.4412 - val_acc: 0.8846\n",
            "Epoch 42/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1051 - acc: 0.9638 - val_loss: 0.4789 - val_acc: 0.8462\n",
            "Epoch 43/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1096 - acc: 0.9619 - val_loss: 0.8424 - val_acc: 0.8120\n",
            "Epoch 44/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0966 - acc: 0.9619 - val_loss: 0.4371 - val_acc: 0.8803\n",
            "Epoch 45/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0798 - acc: 0.9705 - val_loss: 0.5402 - val_acc: 0.8590\n",
            "3 Fold ACC of CNN = 0.9102564102564102\n",
            "\n",
            "4 Fold Training.....\n",
            "Epoch 1/100\n",
            "66/66 [==============================] - 2s 20ms/step - loss: 0.7787 - acc: 0.6992 - val_loss: 0.8385 - val_acc: 0.5897\n",
            "Epoch 2/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.4726 - acc: 0.8206 - val_loss: 0.7577 - val_acc: 0.6624\n",
            "Epoch 3/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.4315 - acc: 0.8434 - val_loss: 0.3781 - val_acc: 0.8632\n",
            "Epoch 4/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.3746 - acc: 0.8605 - val_loss: 0.4318 - val_acc: 0.8205\n",
            "Epoch 5/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.3855 - acc: 0.8586 - val_loss: 0.3973 - val_acc: 0.8632\n",
            "Epoch 6/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.3434 - acc: 0.8677 - val_loss: 0.3586 - val_acc: 0.8718\n",
            "Epoch 7/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.3344 - acc: 0.8748 - val_loss: 0.2711 - val_acc: 0.8932\n",
            "Epoch 8/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.3197 - acc: 0.8810 - val_loss: 0.3709 - val_acc: 0.8761\n",
            "Epoch 9/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.2951 - acc: 0.8901 - val_loss: 0.3190 - val_acc: 0.8974\n",
            "Epoch 10/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2792 - acc: 0.8967 - val_loss: 0.3829 - val_acc: 0.8632\n",
            "Epoch 11/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2861 - acc: 0.8953 - val_loss: 0.3484 - val_acc: 0.8761\n",
            "Epoch 12/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.2766 - acc: 0.8877 - val_loss: 0.2865 - val_acc: 0.8761\n",
            "Epoch 13/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.2451 - acc: 0.9105 - val_loss: 0.2605 - val_acc: 0.9145\n",
            "Epoch 14/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2389 - acc: 0.9139 - val_loss: 0.2976 - val_acc: 0.9060\n",
            "Epoch 15/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2312 - acc: 0.9081 - val_loss: 0.3343 - val_acc: 0.8974\n",
            "Epoch 16/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2180 - acc: 0.9167 - val_loss: 0.3079 - val_acc: 0.8932\n",
            "Epoch 17/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2088 - acc: 0.9191 - val_loss: 0.3672 - val_acc: 0.8718\n",
            "Epoch 18/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2065 - acc: 0.9186 - val_loss: 0.3123 - val_acc: 0.8718\n",
            "Epoch 19/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2055 - acc: 0.9196 - val_loss: 0.3502 - val_acc: 0.8803\n",
            "Epoch 20/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1861 - acc: 0.9296 - val_loss: 0.3298 - val_acc: 0.9017\n",
            "Epoch 21/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1803 - acc: 0.9338 - val_loss: 0.2855 - val_acc: 0.8889\n",
            "Epoch 22/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1717 - acc: 0.9372 - val_loss: 0.3945 - val_acc: 0.8675\n",
            "Epoch 23/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1604 - acc: 0.9405 - val_loss: 0.3067 - val_acc: 0.9017\n",
            "Epoch 24/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1623 - acc: 0.9400 - val_loss: 0.4317 - val_acc: 0.8761\n",
            "Epoch 25/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1668 - acc: 0.9329 - val_loss: 0.4258 - val_acc: 0.8846\n",
            "Epoch 26/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1296 - acc: 0.9524 - val_loss: 0.3622 - val_acc: 0.8675\n",
            "Epoch 27/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1432 - acc: 0.9457 - val_loss: 0.4694 - val_acc: 0.8504\n",
            "Epoch 28/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1706 - acc: 0.9362 - val_loss: 0.4725 - val_acc: 0.8504\n",
            "Epoch 29/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1447 - acc: 0.9462 - val_loss: 0.4011 - val_acc: 0.8803\n",
            "Epoch 30/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1180 - acc: 0.9567 - val_loss: 0.4030 - val_acc: 0.8718\n",
            "Epoch 31/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1124 - acc: 0.9600 - val_loss: 0.4279 - val_acc: 0.8761\n",
            "Epoch 32/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1386 - acc: 0.9495 - val_loss: 0.4094 - val_acc: 0.8675\n",
            "Epoch 33/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1324 - acc: 0.9515 - val_loss: 0.3620 - val_acc: 0.8932\n",
            "Epoch 34/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1025 - acc: 0.9672 - val_loss: 0.4921 - val_acc: 0.8376\n",
            "Epoch 35/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.0880 - acc: 0.9695 - val_loss: 0.4251 - val_acc: 0.8504\n",
            "Epoch 36/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0843 - acc: 0.9724 - val_loss: 0.4515 - val_acc: 0.8632\n",
            "Epoch 37/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0978 - acc: 0.9629 - val_loss: 0.3555 - val_acc: 0.8889\n",
            "Epoch 38/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1061 - acc: 0.9643 - val_loss: 0.3726 - val_acc: 0.8974\n",
            "Epoch 39/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1041 - acc: 0.9624 - val_loss: 0.4038 - val_acc: 0.8718\n",
            "Epoch 40/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0811 - acc: 0.9714 - val_loss: 0.4263 - val_acc: 0.8590\n",
            "Epoch 41/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0722 - acc: 0.9719 - val_loss: 0.5330 - val_acc: 0.8590\n",
            "Epoch 42/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0954 - acc: 0.9657 - val_loss: 0.4756 - val_acc: 0.8761\n",
            "Epoch 43/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.0805 - acc: 0.9695 - val_loss: 0.5035 - val_acc: 0.8675\n",
            "4 Fold ACC of CNN = 0.9145299145299145\n",
            "\n",
            "5 Fold Training.....\n",
            "Epoch 1/100\n",
            "66/66 [==============================] - 2s 20ms/step - loss: 0.7428 - acc: 0.7258 - val_loss: 0.7926 - val_acc: 0.6453\n",
            "Epoch 2/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.4733 - acc: 0.8234 - val_loss: 0.5872 - val_acc: 0.7607\n",
            "Epoch 3/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.4127 - acc: 0.8444 - val_loss: 0.4003 - val_acc: 0.8590\n",
            "Epoch 4/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.3892 - acc: 0.8539 - val_loss: 0.3698 - val_acc: 0.8718\n",
            "Epoch 5/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.3651 - acc: 0.8629 - val_loss: 0.4104 - val_acc: 0.8504\n",
            "Epoch 6/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.3344 - acc: 0.8782 - val_loss: 0.4124 - val_acc: 0.8547\n",
            "Epoch 7/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.3325 - acc: 0.8753 - val_loss: 0.3510 - val_acc: 0.8590\n",
            "Epoch 8/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.3226 - acc: 0.8724 - val_loss: 0.3279 - val_acc: 0.8932\n",
            "Epoch 9/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2898 - acc: 0.8915 - val_loss: 0.3553 - val_acc: 0.8761\n",
            "Epoch 10/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2743 - acc: 0.9034 - val_loss: 0.3569 - val_acc: 0.8761\n",
            "Epoch 11/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2793 - acc: 0.8920 - val_loss: 0.3810 - val_acc: 0.8590\n",
            "Epoch 12/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2734 - acc: 0.9029 - val_loss: 0.3457 - val_acc: 0.8932\n",
            "Epoch 13/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2415 - acc: 0.9105 - val_loss: 0.3525 - val_acc: 0.8718\n",
            "Epoch 14/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2425 - acc: 0.9077 - val_loss: 0.3635 - val_acc: 0.8718\n",
            "Epoch 15/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2337 - acc: 0.9115 - val_loss: 0.3982 - val_acc: 0.8846\n",
            "Epoch 16/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2220 - acc: 0.9134 - val_loss: 0.3396 - val_acc: 0.8846\n",
            "Epoch 17/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2479 - acc: 0.9048 - val_loss: 0.4472 - val_acc: 0.8376\n",
            "Epoch 18/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1992 - acc: 0.9272 - val_loss: 0.4458 - val_acc: 0.8590\n",
            "Epoch 19/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2149 - acc: 0.9219 - val_loss: 0.4320 - val_acc: 0.8462\n",
            "Epoch 20/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2021 - acc: 0.9257 - val_loss: 0.3999 - val_acc: 0.8761\n",
            "Epoch 21/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1995 - acc: 0.9253 - val_loss: 0.3744 - val_acc: 0.8675\n",
            "Epoch 22/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1674 - acc: 0.9372 - val_loss: 0.4415 - val_acc: 0.8504\n",
            "Epoch 23/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1855 - acc: 0.9343 - val_loss: 0.4680 - val_acc: 0.8376\n",
            "Epoch 24/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1651 - acc: 0.9338 - val_loss: 0.4528 - val_acc: 0.8547\n",
            "Epoch 25/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1608 - acc: 0.9438 - val_loss: 0.4163 - val_acc: 0.8803\n",
            "Epoch 26/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1357 - acc: 0.9453 - val_loss: 0.5936 - val_acc: 0.8205\n",
            "Epoch 27/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1559 - acc: 0.9415 - val_loss: 0.4616 - val_acc: 0.8462\n",
            "Epoch 28/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1685 - acc: 0.9400 - val_loss: 0.5272 - val_acc: 0.8291\n",
            "Epoch 29/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1400 - acc: 0.9434 - val_loss: 0.4252 - val_acc: 0.8803\n",
            "Epoch 30/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1201 - acc: 0.9614 - val_loss: 0.4422 - val_acc: 0.8504\n",
            "Epoch 31/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1128 - acc: 0.9629 - val_loss: 0.5025 - val_acc: 0.8547\n",
            "Epoch 32/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1148 - acc: 0.9562 - val_loss: 0.4550 - val_acc: 0.8547\n",
            "Epoch 33/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1406 - acc: 0.9505 - val_loss: 0.4774 - val_acc: 0.8547\n",
            "Epoch 34/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1233 - acc: 0.9553 - val_loss: 0.4842 - val_acc: 0.8462\n",
            "Epoch 35/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0890 - acc: 0.9681 - val_loss: 0.5556 - val_acc: 0.8376\n",
            "Epoch 36/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0824 - acc: 0.9724 - val_loss: 0.4473 - val_acc: 0.8462\n",
            "Epoch 37/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0853 - acc: 0.9700 - val_loss: 0.5320 - val_acc: 0.8547\n",
            "Epoch 38/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0823 - acc: 0.9714 - val_loss: 0.5917 - val_acc: 0.8333\n",
            "5 Fold ACC of CNN = 0.8931623931623932\n",
            "\n",
            "6 Fold Training.....\n",
            "Epoch 1/100\n",
            "66/66 [==============================] - 2s 20ms/step - loss: 0.7790 - acc: 0.6993 - val_loss: 1.1928 - val_acc: 0.5536\n",
            "Epoch 2/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.4807 - acc: 0.8192 - val_loss: 0.5487 - val_acc: 0.7811\n",
            "Epoch 3/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.4360 - acc: 0.8382 - val_loss: 0.4834 - val_acc: 0.8240\n",
            "Epoch 4/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.3731 - acc: 0.8639 - val_loss: 0.4707 - val_acc: 0.8240\n",
            "Epoch 5/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.3678 - acc: 0.8687 - val_loss: 0.4424 - val_acc: 0.8197\n",
            "Epoch 6/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.3375 - acc: 0.8706 - val_loss: 0.3828 - val_acc: 0.8584\n",
            "Epoch 7/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.3102 - acc: 0.8882 - val_loss: 0.3728 - val_acc: 0.8755\n",
            "Epoch 8/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2988 - acc: 0.8877 - val_loss: 0.4417 - val_acc: 0.8455\n",
            "Epoch 9/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.3026 - acc: 0.8911 - val_loss: 0.4620 - val_acc: 0.8326\n",
            "Epoch 10/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2796 - acc: 0.8915 - val_loss: 0.4503 - val_acc: 0.8326\n",
            "Epoch 11/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2726 - acc: 0.8996 - val_loss: 0.4936 - val_acc: 0.8326\n",
            "Epoch 12/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2736 - acc: 0.8911 - val_loss: 0.5498 - val_acc: 0.8155\n",
            "Epoch 13/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2441 - acc: 0.9106 - val_loss: 0.4270 - val_acc: 0.8541\n",
            "Epoch 14/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2325 - acc: 0.9144 - val_loss: 0.3777 - val_acc: 0.8627\n",
            "Epoch 15/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2326 - acc: 0.9134 - val_loss: 0.4161 - val_acc: 0.8541\n",
            "Epoch 16/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2173 - acc: 0.9220 - val_loss: 0.3611 - val_acc: 0.8627\n",
            "Epoch 17/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1976 - acc: 0.9244 - val_loss: 0.4809 - val_acc: 0.8498\n",
            "Epoch 18/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2013 - acc: 0.9239 - val_loss: 0.5817 - val_acc: 0.8240\n",
            "Epoch 19/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2073 - acc: 0.9196 - val_loss: 0.3760 - val_acc: 0.8627\n",
            "Epoch 20/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1651 - acc: 0.9396 - val_loss: 0.4944 - val_acc: 0.8283\n",
            "Epoch 21/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1676 - acc: 0.9396 - val_loss: 0.4915 - val_acc: 0.8197\n",
            "Epoch 22/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1789 - acc: 0.9372 - val_loss: 0.4206 - val_acc: 0.8283\n",
            "Epoch 23/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1705 - acc: 0.9358 - val_loss: 0.3873 - val_acc: 0.8326\n",
            "Epoch 24/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1585 - acc: 0.9420 - val_loss: 0.4123 - val_acc: 0.8541\n",
            "Epoch 25/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1394 - acc: 0.9543 - val_loss: 0.4606 - val_acc: 0.8369\n",
            "Epoch 26/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1789 - acc: 0.9353 - val_loss: 0.4878 - val_acc: 0.8412\n",
            "Epoch 27/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1312 - acc: 0.9548 - val_loss: 0.5145 - val_acc: 0.8412\n",
            "Epoch 28/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1310 - acc: 0.9543 - val_loss: 0.5841 - val_acc: 0.8197\n",
            "Epoch 29/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1194 - acc: 0.9600 - val_loss: 0.5492 - val_acc: 0.8240\n",
            "Epoch 30/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1150 - acc: 0.9558 - val_loss: 0.6345 - val_acc: 0.8112\n",
            "Epoch 31/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1214 - acc: 0.9520 - val_loss: 0.4274 - val_acc: 0.8455\n",
            "Epoch 32/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1275 - acc: 0.9524 - val_loss: 0.5252 - val_acc: 0.8283\n",
            "Epoch 33/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1175 - acc: 0.9567 - val_loss: 0.8537 - val_acc: 0.8026\n",
            "Epoch 34/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1056 - acc: 0.9624 - val_loss: 0.5418 - val_acc: 0.8412\n",
            "Epoch 35/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0800 - acc: 0.9753 - val_loss: 0.5181 - val_acc: 0.8369\n",
            "Epoch 36/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1092 - acc: 0.9586 - val_loss: 0.5229 - val_acc: 0.8541\n",
            "Epoch 37/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1056 - acc: 0.9619 - val_loss: 0.5243 - val_acc: 0.8455\n",
            "6 Fold ACC of CNN = 0.8755364806866953\n",
            "\n",
            "7 Fold Training.....\n",
            "Epoch 1/100\n",
            "66/66 [==============================] - 2s 19ms/step - loss: 0.7980 - acc: 0.6775 - val_loss: 0.6821 - val_acc: 0.7597\n",
            "Epoch 2/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.4969 - acc: 0.8111 - val_loss: 0.5790 - val_acc: 0.7768\n",
            "Epoch 3/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.4194 - acc: 0.8378 - val_loss: 0.6411 - val_acc: 0.7511\n",
            "Epoch 4/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.3783 - acc: 0.8587 - val_loss: 0.4162 - val_acc: 0.8498\n",
            "Epoch 5/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.3611 - acc: 0.8649 - val_loss: 0.3811 - val_acc: 0.8498\n",
            "Epoch 6/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.3440 - acc: 0.8716 - val_loss: 0.3549 - val_acc: 0.8455\n",
            "Epoch 7/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.3333 - acc: 0.8730 - val_loss: 0.3791 - val_acc: 0.8670\n",
            "Epoch 8/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2904 - acc: 0.8911 - val_loss: 0.3940 - val_acc: 0.8584\n",
            "Epoch 9/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2802 - acc: 0.8906 - val_loss: 0.4121 - val_acc: 0.8712\n",
            "Epoch 10/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2690 - acc: 0.9001 - val_loss: 0.5164 - val_acc: 0.8541\n",
            "Epoch 11/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2668 - acc: 0.8982 - val_loss: 0.4211 - val_acc: 0.8755\n",
            "Epoch 12/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2298 - acc: 0.9220 - val_loss: 0.4386 - val_acc: 0.8369\n",
            "Epoch 13/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2427 - acc: 0.9077 - val_loss: 0.6124 - val_acc: 0.7940\n",
            "Epoch 14/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2392 - acc: 0.9039 - val_loss: 0.4480 - val_acc: 0.8584\n",
            "Epoch 15/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2351 - acc: 0.9125 - val_loss: 0.5003 - val_acc: 0.8584\n",
            "Epoch 16/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2249 - acc: 0.9148 - val_loss: 0.4118 - val_acc: 0.8627\n",
            "Epoch 17/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2007 - acc: 0.9225 - val_loss: 0.4807 - val_acc: 0.8712\n",
            "Epoch 18/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1846 - acc: 0.9310 - val_loss: 0.4585 - val_acc: 0.8627\n",
            "Epoch 19/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1949 - acc: 0.9248 - val_loss: 0.4874 - val_acc: 0.8670\n",
            "Epoch 20/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1849 - acc: 0.9310 - val_loss: 0.5010 - val_acc: 0.8455\n",
            "Epoch 21/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1638 - acc: 0.9434 - val_loss: 0.6476 - val_acc: 0.8283\n",
            "Epoch 22/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1483 - acc: 0.9481 - val_loss: 0.4646 - val_acc: 0.8369\n",
            "Epoch 23/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1526 - acc: 0.9439 - val_loss: 0.5480 - val_acc: 0.8584\n",
            "Epoch 24/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1299 - acc: 0.9553 - val_loss: 0.5073 - val_acc: 0.8541\n",
            "Epoch 25/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1656 - acc: 0.9377 - val_loss: 0.5440 - val_acc: 0.8455\n",
            "Epoch 26/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1515 - acc: 0.9415 - val_loss: 0.4947 - val_acc: 0.8541\n",
            "Epoch 27/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1315 - acc: 0.9534 - val_loss: 0.4606 - val_acc: 0.8712\n",
            "Epoch 28/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1212 - acc: 0.9581 - val_loss: 0.6594 - val_acc: 0.8455\n",
            "Epoch 29/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1414 - acc: 0.9481 - val_loss: 0.5309 - val_acc: 0.8498\n",
            "Epoch 30/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1169 - acc: 0.9572 - val_loss: 0.5919 - val_acc: 0.8584\n",
            "Epoch 31/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1109 - acc: 0.9596 - val_loss: 0.5255 - val_acc: 0.8627\n",
            "Epoch 32/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1054 - acc: 0.9619 - val_loss: 0.5770 - val_acc: 0.8712\n",
            "Epoch 33/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0998 - acc: 0.9638 - val_loss: 0.6528 - val_acc: 0.8455\n",
            "Epoch 34/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0963 - acc: 0.9581 - val_loss: 0.5090 - val_acc: 0.8455\n",
            "Epoch 35/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1084 - acc: 0.9634 - val_loss: 0.7796 - val_acc: 0.8155\n",
            "Epoch 36/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.0969 - acc: 0.9643 - val_loss: 0.5677 - val_acc: 0.8627\n",
            "Epoch 37/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1054 - acc: 0.9567 - val_loss: 0.5759 - val_acc: 0.8755\n",
            "Epoch 38/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0728 - acc: 0.9762 - val_loss: 0.6334 - val_acc: 0.8627\n",
            "Epoch 39/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0801 - acc: 0.9719 - val_loss: 0.7553 - val_acc: 0.8240\n",
            "Epoch 40/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0698 - acc: 0.9786 - val_loss: 0.7570 - val_acc: 0.8240\n",
            "Epoch 41/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.0818 - acc: 0.9700 - val_loss: 0.6552 - val_acc: 0.8712\n",
            "7 Fold ACC of CNN = 0.8755364806866953\n",
            "\n",
            "8 Fold Training.....\n",
            "Epoch 1/100\n",
            "66/66 [==============================] - 2s 19ms/step - loss: 0.7523 - acc: 0.7122 - val_loss: 0.8585 - val_acc: 0.6481\n",
            "Epoch 2/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.4790 - acc: 0.8235 - val_loss: 0.7879 - val_acc: 0.7124\n",
            "Epoch 3/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.4024 - acc: 0.8511 - val_loss: 0.5940 - val_acc: 0.7768\n",
            "Epoch 4/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.4100 - acc: 0.8568 - val_loss: 0.5133 - val_acc: 0.8326\n",
            "Epoch 5/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.3479 - acc: 0.8725 - val_loss: 0.4339 - val_acc: 0.8455\n",
            "Epoch 6/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.3272 - acc: 0.8844 - val_loss: 0.5596 - val_acc: 0.8240\n",
            "Epoch 7/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.3040 - acc: 0.8877 - val_loss: 0.4713 - val_acc: 0.8283\n",
            "Epoch 8/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.3112 - acc: 0.8882 - val_loss: 0.4599 - val_acc: 0.8627\n",
            "Epoch 9/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.2767 - acc: 0.8968 - val_loss: 0.4403 - val_acc: 0.8541\n",
            "Epoch 10/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2785 - acc: 0.8977 - val_loss: 0.5841 - val_acc: 0.8369\n",
            "Epoch 11/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2383 - acc: 0.9139 - val_loss: 0.5376 - val_acc: 0.8283\n",
            "Epoch 12/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.2705 - acc: 0.8944 - val_loss: 0.5984 - val_acc: 0.7983\n",
            "Epoch 13/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.2467 - acc: 0.9058 - val_loss: 0.4632 - val_acc: 0.8670\n",
            "Epoch 14/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2414 - acc: 0.9106 - val_loss: 0.5372 - val_acc: 0.8283\n",
            "Epoch 15/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2256 - acc: 0.9134 - val_loss: 0.5198 - val_acc: 0.8455\n",
            "Epoch 16/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2058 - acc: 0.9229 - val_loss: 0.6199 - val_acc: 0.8369\n",
            "Epoch 17/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1866 - acc: 0.9272 - val_loss: 0.5092 - val_acc: 0.8712\n",
            "Epoch 18/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2086 - acc: 0.9244 - val_loss: 0.8454 - val_acc: 0.7725\n",
            "Epoch 19/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2166 - acc: 0.9220 - val_loss: 0.5823 - val_acc: 0.8283\n",
            "Epoch 20/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1768 - acc: 0.9334 - val_loss: 0.5794 - val_acc: 0.8283\n",
            "Epoch 21/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.2023 - acc: 0.9239 - val_loss: 0.6330 - val_acc: 0.8326\n",
            "Epoch 22/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1827 - acc: 0.9348 - val_loss: 0.6203 - val_acc: 0.8026\n",
            "Epoch 23/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1729 - acc: 0.9315 - val_loss: 0.5832 - val_acc: 0.8498\n",
            "Epoch 24/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1477 - acc: 0.9472 - val_loss: 0.5910 - val_acc: 0.8369\n",
            "Epoch 25/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1401 - acc: 0.9496 - val_loss: 0.6248 - val_acc: 0.8283\n",
            "Epoch 26/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1714 - acc: 0.9320 - val_loss: 0.6905 - val_acc: 0.8112\n",
            "Epoch 27/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1528 - acc: 0.9363 - val_loss: 0.5489 - val_acc: 0.8455\n",
            "Epoch 28/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1169 - acc: 0.9562 - val_loss: 0.6041 - val_acc: 0.8412\n",
            "Epoch 29/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1252 - acc: 0.9562 - val_loss: 0.5992 - val_acc: 0.8498\n",
            "Epoch 30/100\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.1229 - acc: 0.9562 - val_loss: 0.6715 - val_acc: 0.8584\n",
            "Epoch 31/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1222 - acc: 0.9534 - val_loss: 0.6531 - val_acc: 0.8455\n",
            "Epoch 32/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1046 - acc: 0.9615 - val_loss: 0.7937 - val_acc: 0.8369\n",
            "Epoch 33/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0997 - acc: 0.9624 - val_loss: 0.7167 - val_acc: 0.8326\n",
            "Epoch 34/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1447 - acc: 0.9443 - val_loss: 0.6880 - val_acc: 0.8541\n",
            "Epoch 35/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1159 - acc: 0.9581 - val_loss: 0.6745 - val_acc: 0.8240\n",
            "Epoch 36/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1281 - acc: 0.9548 - val_loss: 0.7903 - val_acc: 0.7983\n",
            "Epoch 37/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0804 - acc: 0.9676 - val_loss: 0.6823 - val_acc: 0.8455\n",
            "Epoch 38/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1225 - acc: 0.9534 - val_loss: 0.6390 - val_acc: 0.8584\n",
            "Epoch 39/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0923 - acc: 0.9676 - val_loss: 0.7612 - val_acc: 0.8197\n",
            "Epoch 40/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0690 - acc: 0.9762 - val_loss: 0.7544 - val_acc: 0.8369\n",
            "Epoch 41/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.0593 - acc: 0.9800 - val_loss: 0.7420 - val_acc: 0.8326\n",
            "Epoch 42/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0794 - acc: 0.9662 - val_loss: 0.9153 - val_acc: 0.7897\n",
            "Epoch 43/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0839 - acc: 0.9705 - val_loss: 0.8391 - val_acc: 0.8455\n",
            "Epoch 44/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0678 - acc: 0.9757 - val_loss: 0.7426 - val_acc: 0.8369\n",
            "Epoch 45/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.0705 - acc: 0.9729 - val_loss: 0.8091 - val_acc: 0.8455\n",
            "Epoch 46/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.0731 - acc: 0.9738 - val_loss: 0.9038 - val_acc: 0.8197\n",
            "Epoch 47/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.0753 - acc: 0.9734 - val_loss: 0.6775 - val_acc: 0.8755\n",
            "Epoch 48/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0531 - acc: 0.9833 - val_loss: 0.7687 - val_acc: 0.8412\n",
            "Epoch 49/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0744 - acc: 0.9719 - val_loss: 0.7954 - val_acc: 0.8455\n",
            "Epoch 50/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0664 - acc: 0.9791 - val_loss: 0.8674 - val_acc: 0.8412\n",
            "Epoch 51/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0599 - acc: 0.9781 - val_loss: 0.8759 - val_acc: 0.8541\n",
            "Epoch 52/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.0644 - acc: 0.9748 - val_loss: 0.7776 - val_acc: 0.8541\n",
            "Epoch 53/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0538 - acc: 0.9781 - val_loss: 0.7845 - val_acc: 0.8369\n",
            "Epoch 54/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0999 - acc: 0.9615 - val_loss: 0.9467 - val_acc: 0.7983\n",
            "Epoch 55/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0821 - acc: 0.9681 - val_loss: 0.8720 - val_acc: 0.8412\n",
            "Epoch 56/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0607 - acc: 0.9767 - val_loss: 0.8138 - val_acc: 0.8412\n",
            "Epoch 57/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0490 - acc: 0.9824 - val_loss: 0.7885 - val_acc: 0.8369\n",
            "Epoch 58/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0739 - acc: 0.9738 - val_loss: 0.7357 - val_acc: 0.8326\n",
            "Epoch 59/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0832 - acc: 0.9700 - val_loss: 0.8299 - val_acc: 0.8455\n",
            "Epoch 60/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0514 - acc: 0.9824 - val_loss: 0.8421 - val_acc: 0.8455\n",
            "Epoch 61/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0546 - acc: 0.9819 - val_loss: 0.8013 - val_acc: 0.8498\n",
            "Epoch 62/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0549 - acc: 0.9795 - val_loss: 0.9309 - val_acc: 0.8283\n",
            "Epoch 63/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0638 - acc: 0.9767 - val_loss: 0.8312 - val_acc: 0.8498\n",
            "Epoch 64/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0332 - acc: 0.9895 - val_loss: 0.8098 - val_acc: 0.8326\n",
            "Epoch 65/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.0331 - acc: 0.9881 - val_loss: 1.1088 - val_acc: 0.8412\n",
            "Epoch 66/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.0383 - acc: 0.9862 - val_loss: 0.9498 - val_acc: 0.8455\n",
            "Epoch 67/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0434 - acc: 0.9872 - val_loss: 0.9214 - val_acc: 0.8369\n",
            "Epoch 68/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.0496 - acc: 0.9833 - val_loss: 0.7956 - val_acc: 0.8369\n",
            "Epoch 69/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0269 - acc: 0.9924 - val_loss: 0.8535 - val_acc: 0.8498\n",
            "Epoch 70/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.0503 - acc: 0.9810 - val_loss: 0.8438 - val_acc: 0.8541\n",
            "Epoch 71/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0336 - acc: 0.9862 - val_loss: 0.9278 - val_acc: 0.8455\n",
            "Epoch 72/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0431 - acc: 0.9857 - val_loss: 0.9018 - val_acc: 0.8155\n",
            "Epoch 73/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0512 - acc: 0.9829 - val_loss: 0.8641 - val_acc: 0.8412\n",
            "Epoch 74/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0558 - acc: 0.9810 - val_loss: 0.9640 - val_acc: 0.8412\n",
            "Epoch 75/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.0450 - acc: 0.9819 - val_loss: 0.9851 - val_acc: 0.8498\n",
            "Epoch 76/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.0406 - acc: 0.9881 - val_loss: 1.1055 - val_acc: 0.8326\n",
            "Epoch 77/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.0433 - acc: 0.9838 - val_loss: 0.8920 - val_acc: 0.8369\n",
            "8 Fold ACC of CNN = 0.8755364806866953\n",
            "\n",
            "9 Fold Training.....\n",
            "Epoch 1/100\n",
            "66/66 [==============================] - 2s 21ms/step - loss: 0.7536 - acc: 0.7136 - val_loss: 1.0988 - val_acc: 0.4635\n",
            "Epoch 2/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.4914 - acc: 0.8130 - val_loss: 0.7455 - val_acc: 0.6996\n",
            "Epoch 3/100\n",
            "66/66 [==============================] - 1s 18ms/step - loss: 0.4418 - acc: 0.8302 - val_loss: 0.5033 - val_acc: 0.7983\n",
            "Epoch 4/100\n",
            "66/66 [==============================] - 1s 18ms/step - loss: 0.4084 - acc: 0.8473 - val_loss: 0.3765 - val_acc: 0.8412\n",
            "Epoch 5/100\n",
            "66/66 [==============================] - 1s 18ms/step - loss: 0.3464 - acc: 0.8701 - val_loss: 0.3890 - val_acc: 0.8498\n",
            "Epoch 6/100\n",
            "66/66 [==============================] - 1s 18ms/step - loss: 0.3428 - acc: 0.8730 - val_loss: 0.3745 - val_acc: 0.8584\n",
            "Epoch 7/100\n",
            "66/66 [==============================] - 1s 18ms/step - loss: 0.3127 - acc: 0.8849 - val_loss: 0.4315 - val_acc: 0.8498\n",
            "Epoch 8/100\n",
            "66/66 [==============================] - 1s 20ms/step - loss: 0.3138 - acc: 0.8820 - val_loss: 0.3719 - val_acc: 0.8627\n",
            "Epoch 9/100\n",
            "66/66 [==============================] - 1s 18ms/step - loss: 0.2860 - acc: 0.8963 - val_loss: 0.3448 - val_acc: 0.8627\n",
            "Epoch 10/100\n",
            "66/66 [==============================] - 1s 19ms/step - loss: 0.2669 - acc: 0.8982 - val_loss: 0.4161 - val_acc: 0.8412\n",
            "Epoch 11/100\n",
            "66/66 [==============================] - 1s 18ms/step - loss: 0.2738 - acc: 0.8934 - val_loss: 0.4074 - val_acc: 0.8627\n",
            "Epoch 12/100\n",
            "66/66 [==============================] - 1s 19ms/step - loss: 0.2373 - acc: 0.9110 - val_loss: 0.3592 - val_acc: 0.8798\n",
            "Epoch 13/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2549 - acc: 0.9072 - val_loss: 0.3847 - val_acc: 0.8627\n",
            "Epoch 14/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.2450 - acc: 0.9077 - val_loss: 0.3555 - val_acc: 0.8670\n",
            "Epoch 15/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.2285 - acc: 0.9134 - val_loss: 0.4393 - val_acc: 0.8369\n",
            "Epoch 16/100\n",
            "66/66 [==============================] - 1s 18ms/step - loss: 0.1909 - acc: 0.9315 - val_loss: 0.4354 - val_acc: 0.8541\n",
            "Epoch 17/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1942 - acc: 0.9253 - val_loss: 0.3865 - val_acc: 0.8627\n",
            "Epoch 18/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1961 - acc: 0.9282 - val_loss: 0.4848 - val_acc: 0.8326\n",
            "Epoch 19/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2006 - acc: 0.9191 - val_loss: 0.3479 - val_acc: 0.8798\n",
            "Epoch 20/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1811 - acc: 0.9315 - val_loss: 0.4070 - val_acc: 0.8670\n",
            "Epoch 21/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1769 - acc: 0.9363 - val_loss: 0.4369 - val_acc: 0.8627\n",
            "Epoch 22/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1542 - acc: 0.9458 - val_loss: 0.4752 - val_acc: 0.8670\n",
            "Epoch 23/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1517 - acc: 0.9367 - val_loss: 0.5191 - val_acc: 0.8197\n",
            "Epoch 24/100\n",
            "66/66 [==============================] - 1s 18ms/step - loss: 0.1467 - acc: 0.9491 - val_loss: 0.4202 - val_acc: 0.8541\n",
            "Epoch 25/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1416 - acc: 0.9443 - val_loss: 0.4944 - val_acc: 0.8283\n",
            "Epoch 26/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1640 - acc: 0.9367 - val_loss: 0.5698 - val_acc: 0.8369\n",
            "Epoch 27/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1722 - acc: 0.9343 - val_loss: 0.4934 - val_acc: 0.8755\n",
            "Epoch 28/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1561 - acc: 0.9434 - val_loss: 0.5467 - val_acc: 0.8798\n",
            "Epoch 29/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1422 - acc: 0.9410 - val_loss: 0.5090 - val_acc: 0.8455\n",
            "Epoch 30/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1275 - acc: 0.9567 - val_loss: 0.5009 - val_acc: 0.8412\n",
            "Epoch 31/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1428 - acc: 0.9401 - val_loss: 0.4487 - val_acc: 0.8798\n",
            "Epoch 32/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1156 - acc: 0.9586 - val_loss: 0.4840 - val_acc: 0.8584\n",
            "Epoch 33/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1139 - acc: 0.9572 - val_loss: 0.4955 - val_acc: 0.8584\n",
            "Epoch 34/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.0941 - acc: 0.9648 - val_loss: 0.4534 - val_acc: 0.8541\n",
            "Epoch 35/100\n",
            "66/66 [==============================] - 1s 18ms/step - loss: 0.0893 - acc: 0.9681 - val_loss: 0.5162 - val_acc: 0.8498\n",
            "Epoch 36/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.0883 - acc: 0.9638 - val_loss: 0.5298 - val_acc: 0.8498\n",
            "Epoch 37/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.0882 - acc: 0.9681 - val_loss: 0.6701 - val_acc: 0.8412\n",
            "Epoch 38/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.0815 - acc: 0.9715 - val_loss: 0.5331 - val_acc: 0.8455\n",
            "Epoch 39/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.0978 - acc: 0.9605 - val_loss: 0.4872 - val_acc: 0.8798\n",
            "Epoch 40/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.0801 - acc: 0.9743 - val_loss: 0.5117 - val_acc: 0.8498\n",
            "Epoch 41/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1047 - acc: 0.9624 - val_loss: 0.7164 - val_acc: 0.8240\n",
            "Epoch 42/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.0869 - acc: 0.9672 - val_loss: 0.6073 - val_acc: 0.8326\n",
            "9 Fold ACC of CNN = 0.8798283261802575\n",
            "\n",
            "10 Fold Training.....\n",
            "Epoch 1/100\n",
            "66/66 [==============================] - 2s 20ms/step - loss: 0.7599 - acc: 0.7022 - val_loss: 0.8101 - val_acc: 0.6309\n",
            "Epoch 2/100\n",
            "66/66 [==============================] - 1s 18ms/step - loss: 0.4800 - acc: 0.8206 - val_loss: 0.6558 - val_acc: 0.7597\n",
            "Epoch 3/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.4209 - acc: 0.8406 - val_loss: 0.4202 - val_acc: 0.8455\n",
            "Epoch 4/100\n",
            "66/66 [==============================] - 1s 18ms/step - loss: 0.3829 - acc: 0.8668 - val_loss: 0.3672 - val_acc: 0.8627\n",
            "Epoch 5/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.3357 - acc: 0.8763 - val_loss: 0.3399 - val_acc: 0.8841\n",
            "Epoch 6/100\n",
            "66/66 [==============================] - 1s 18ms/step - loss: 0.3318 - acc: 0.8749 - val_loss: 0.3228 - val_acc: 0.8884\n",
            "Epoch 7/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.3183 - acc: 0.8825 - val_loss: 0.4672 - val_acc: 0.8283\n",
            "Epoch 8/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2944 - acc: 0.8882 - val_loss: 0.4277 - val_acc: 0.8240\n",
            "Epoch 9/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.2833 - acc: 0.9015 - val_loss: 0.3273 - val_acc: 0.8841\n",
            "Epoch 10/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2797 - acc: 0.8939 - val_loss: 0.3585 - val_acc: 0.8798\n",
            "Epoch 11/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2678 - acc: 0.8972 - val_loss: 0.4069 - val_acc: 0.8369\n",
            "Epoch 12/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2423 - acc: 0.9115 - val_loss: 0.4264 - val_acc: 0.8455\n",
            "Epoch 13/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2606 - acc: 0.9058 - val_loss: 0.3497 - val_acc: 0.8755\n",
            "Epoch 14/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2563 - acc: 0.9053 - val_loss: 0.3606 - val_acc: 0.8841\n",
            "Epoch 15/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2430 - acc: 0.9110 - val_loss: 0.3575 - val_acc: 0.8798\n",
            "Epoch 16/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.2433 - acc: 0.9082 - val_loss: 0.4764 - val_acc: 0.8498\n",
            "Epoch 17/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.2385 - acc: 0.9144 - val_loss: 0.3723 - val_acc: 0.8755\n",
            "Epoch 18/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.2212 - acc: 0.9206 - val_loss: 0.3765 - val_acc: 0.8627\n",
            "Epoch 19/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1816 - acc: 0.9296 - val_loss: 0.4167 - val_acc: 0.8670\n",
            "Epoch 20/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2007 - acc: 0.9229 - val_loss: 0.3957 - val_acc: 0.8627\n",
            "Epoch 21/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.2004 - acc: 0.9282 - val_loss: 0.4151 - val_acc: 0.8798\n",
            "Epoch 22/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1815 - acc: 0.9310 - val_loss: 0.3987 - val_acc: 0.8798\n",
            "Epoch 23/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1479 - acc: 0.9443 - val_loss: 0.3819 - val_acc: 0.8627\n",
            "Epoch 24/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1623 - acc: 0.9353 - val_loss: 0.4122 - val_acc: 0.8884\n",
            "Epoch 25/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1770 - acc: 0.9377 - val_loss: 0.3783 - val_acc: 0.8755\n",
            "Epoch 26/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1569 - acc: 0.9458 - val_loss: 0.4612 - val_acc: 0.8627\n",
            "Epoch 27/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1573 - acc: 0.9443 - val_loss: 0.4418 - val_acc: 0.8627\n",
            "Epoch 28/100\n",
            "66/66 [==============================] - 1s 18ms/step - loss: 0.1609 - acc: 0.9396 - val_loss: 0.4324 - val_acc: 0.8670\n",
            "Epoch 29/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1361 - acc: 0.9515 - val_loss: 0.3945 - val_acc: 0.8798\n",
            "Epoch 30/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1247 - acc: 0.9562 - val_loss: 0.3971 - val_acc: 0.8798\n",
            "Epoch 31/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1431 - acc: 0.9491 - val_loss: 0.3891 - val_acc: 0.8712\n",
            "Epoch 32/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1212 - acc: 0.9577 - val_loss: 0.5002 - val_acc: 0.8584\n",
            "Epoch 33/100\n",
            "66/66 [==============================] - 1s 16ms/step - loss: 0.1367 - acc: 0.9467 - val_loss: 0.3754 - val_acc: 0.8670\n",
            "Epoch 34/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1205 - acc: 0.9553 - val_loss: 0.4701 - val_acc: 0.8670\n",
            "Epoch 35/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1099 - acc: 0.9567 - val_loss: 0.4590 - val_acc: 0.8670\n",
            "Epoch 36/100\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.1110 - acc: 0.9600 - val_loss: 0.4339 - val_acc: 0.8670\n",
            "10 Fold ACC of CNN = 0.8884120171673819\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(cnn_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snVCoj24z2ko",
        "outputId": "763c0551-c475-4976-dcd8-1ceca9c92c90"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8835020725578666"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission['target'] = np.argmax(cnn_pred, axis = 1)\n",
        "submission.to_csv('submission.csv', index = False)\n",
        "submission.target.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkCm8cJ93cWy",
        "outputId": "49dadf3c-b0b6-4f5b-9cc4-44fadf9691e3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    2454\n",
              "1    2393\n",
              "0    2257\n",
              "3    2239\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BcR9uI0ffCtj"
      }
    }
  ]
}